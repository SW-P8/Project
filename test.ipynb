{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DTC.dtc_executor import DTCExecutor\n",
    "from DTC.route_skeleton import RouteSkeleton\n",
    "import json\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#a = DTCExecutor()\n",
    "#pc = a.create_point_cloud_with_n_points(17000000, city=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./pc.pickle', 'wb') as handle:\n",
    "#    pickle.dump(pc, file=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = None\n",
    "with open('./pc.pickle', 'rb') as handle:\n",
    "    pc = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DTC.gridsystem import GridSystem\n",
    "from DTC.construct_safe_area import SafeArea\n",
    "gs = GridSystem(pc)\n",
    "route_skeleton = set()\n",
    "with open(\"./AllcityRSK.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for elem in data:\n",
    "        clean_elem = elem.strip('()').split(',')\n",
    "\n",
    "        set_elem = tuple(float(num.strip()) for num in clean_elem)\n",
    "\n",
    "        route_skeleton.add(set_elem)\n",
    "\n",
    "\n",
    "converted_data = {}\n",
    "with open(\"./AllcitySA.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for key, value in data.items():\n",
    "        \n",
    "        \n",
    "        clean_key = key.strip('()').split(',')\n",
    "        tuple_key = tuple(float(num.strip()) for num in clean_key)\n",
    "\n",
    "        converted_data[tuple_key] = SafeArea(tuple_key, radius=value[0], cardinality=value[1], confidence_change=1, normalisation_factor=0.5, cardinality_squish=0.15, max_confidence_change=0.15)\n",
    "\n",
    "\n",
    "gs.safe_areas = converted_data\n",
    "gs.route_skeleton = route_skeleton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "i = 0\n",
    "for traj in gs.pc.trajectories:\n",
    "    i+=1\n",
    "#old_traj = copy.deepcopy(gs.pc.trajectories)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract r\n",
    "import random\n",
    "traj_for_test = random.sample(gs.pc.trajectories, 1467)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for traj in traj_for_test:\n",
    "    for point in traj.points:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 5% of 52489 - or 5 % of all the points in the collective pointcloud for all trajectories\n",
    "from math import ceil\n",
    "amount_to_shift = ceil(i * 0.05)\n",
    "print(amount_to_shift, \" and \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We use random.sample to get a random set of trajectories every time this runs***\n",
    "- furthermore, we take 5% of the points and shift them between 1-200m randomly \n",
    "- this is done to test the cleaning of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DTC.distance_calculator import DistanceCalculator\n",
    "list_of_points = []\n",
    "for traj in traj_for_test:\n",
    "    for point in traj.points:\n",
    "        print(point.longitude)\n",
    "        list_of_points.append(point)\n",
    "list_of_points = copy.copy(list_of_points)\n",
    "\n",
    "points_to_shift = random.sample(list_of_points, amount_to_shift)\n",
    "print(len(points_to_shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DTC.distance_calculator import DistanceCalculator\n",
    "from DTC.point import Point\n",
    "from datetime import datetime\n",
    "#Shifting the points twice seem to give me coordinate that does not work - this does not really make any sense\n",
    "\n",
    "for point in points_to_shift:\n",
    "    print(point.longitude)\n",
    "    point_shifted = DistanceCalculator.shift_point_with_bearing(point, random.randint(1,200), random.randint(0,359))\n",
    "    point.longitude = point_shifted[0]\n",
    "    point.latitude = point_shifted[1]\n",
    "    point.noise = True\n",
    "    #point = DistanceCalculator.shift_point_with_bearing_and_alter_point(point, random.randint(1,200), DistanceCalculator.EAST, gs.initialization_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for point in points_to_shift:\n",
    "    print(point.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DTC import clean\n",
    "clean_t = clean.CleanTraj(gs.safe_areas, gs.route_skeleton, gs.initialization_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_points = clean_t.clean(list_of_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After Cleaning, make comparison with the old trajectory points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_points))\n",
    "i = 0\n",
    "for point in cleaned_points:\n",
    "    if point == True:\n",
    "        i += 1\n",
    "print(i)\n",
    "i = 0\n",
    "for point in list_of_points:\n",
    "    if point.noise == True:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Setup**\n",
    "\n",
    "- Using the setup from the paper DTC\n",
    "- Using 1472 trajectories as in the paper in the first set of experiments\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "**This is only with a MR with 2million points**\n",
    "\n",
    "*first run*\n",
    "- 926 cleaned\n",
    "- 3107 to clean\n",
    "\n",
    "*second run*\n",
    "- 1140 cleaned\n",
    "- 2564 to clean\n",
    "\n",
    "\n",
    "**Running with 11 million points**\n",
    "\n",
    "*first run*\n",
    "- 739 cleaned\n",
    "- 2573 to clean\n",
    "\n",
    "*second run*\n",
    "- 775 cleaned\n",
    "- 2342 to clean\n",
    "\n",
    "*third run*\n",
    "- 850 cleaned\n",
    "- 2394 to clean\n",
    "\n",
    "*fourth run*\n",
    "- 656 cleaned\n",
    "- 2386 to clean\n",
    "\n",
    "*fifth run*\n",
    "- 1137 cleaned\n",
    "- 3998 to clean\n",
    "\n",
    "- VERY low amount of true positives - thats bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cleaned = sum([739, 775, 850, 656]) / sum([2573, 2342, 2394, 2386])\n",
    "print(average_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
